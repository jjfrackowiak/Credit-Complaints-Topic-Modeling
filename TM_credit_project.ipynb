{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying most frequently reported credit products\n",
    "\n",
    "Direct link to selected data: https://www.consumerfinance.gov/data-research/consumer-complaints/search/api/v1/?date_received_max=2022-11-03&date_received_min=2011-12-01&field=all&format=csv&has_narrative=true&lens=product&no_aggs=true&product=Credit%20reporting%20or%20other%20personal%20consumer%20reports&product=Debt%20collection&product=Mortgage&product=Credit%20card%20or%20prepaid%20card&product=Checking%20or%20savings%20account&product=Student%20loan&product=Credit%20reporting&product=Money%20transfer%2C%20virtual%20currency%2C%20or%20money%20service&size=524341&sub_lens=sub_product&trend_depth=5&trend_interval=month\n",
    "\n",
    "Business task - classifying mails and directing them to proper departments of a bank, performing investingations on unlabeled data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Data processing\n",
    "import nltk\n",
    "import pickle\n",
    "import re\n",
    "import nltk.corpus\n",
    "import random as rand\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints = pd.read_csv('complaints-22-23.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date received', 'Product', 'Sub-product', 'Issue', 'Sub-issue',\n",
       "       'Consumer complaint narrative', 'Company public response', 'Company',\n",
       "       'State', 'ZIP code', 'Tags', 'Consumer consent provided?',\n",
       "       'Submitted via', 'Date sent to company', 'Company response to consumer',\n",
       "       'Timely response?', 'Consumer disputed?', 'Complaint ID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complaints.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product\n",
       "Debt collection                                       194261\n",
       "Mortgage                                               98508\n",
       "Credit card or prepaid card                            83554\n",
       "Checking or savings account                            55647\n",
       "Student loan                                           33519\n",
       "Credit reporting                                       31587\n",
       "Money transfer, virtual currency, or money service     27265\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complaints = complaints[[\"Consumer complaint narrative\", \"Product\"]].copy()\n",
    "complaints['Product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling: Randomly drop rows from majority classes \n",
    "# so that we are left with 30'000 rows for each value\n",
    "most_frequent_products = ['Debt collection',\n",
    "'Mortgage',\n",
    "'Credit card or prepaid card',\n",
    "'Checking or savings account']\n",
    "\n",
    "# Create an empty DataFrame to store the reduced rows\n",
    "complaints_reduced = pd.DataFrame(columns=complaints.columns)\n",
    "\n",
    "# Set the desired number of rows for each distinct value\n",
    "target_rows = 5000\n",
    "\n",
    "# Randomly reduce the rows for each distinct value to the target number\n",
    "for value in most_frequent_products:\n",
    "    # Filter the DataFrame to select rows with the current value\n",
    "    filtered_rows = complaints[complaints['Product'] == value]\n",
    "    \n",
    "    if len(filtered_rows) > target_rows:\n",
    "        # Randomly shuffle the rows and keep the first target_rows rows\n",
    "        shuffled_rows = filtered_rows.sample(frac=1).head(target_rows)\n",
    "    else:\n",
    "        # If there are fewer rows than the target, keep all of them\n",
    "        shuffled_rows = filtered_rows\n",
    "    \n",
    "    # Concatenate the selected rows with the reduced DataFrame\n",
    "    complaints_reduced = pd.concat([complaints_reduced, shuffled_rows])\n",
    "\n",
    "# Reset the index of the reduced DataFrame\n",
    "complaints_reduced = complaints_reduced.reset_index(drop=True)\n",
    "complaints = complaints_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product\n",
       "Debt collection                30000\n",
       "Mortgage                       30000\n",
       "Credit card or prepaid card    30000\n",
       "Checking or savings account    30000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the values\n",
    "complaints_products = complaints[[\"Consumer complaint narrative\", \"Product\"]].copy()\n",
    "complaints_products['Product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to assign labels\n",
    "def assign_label(row):\n",
    "    if row['Product'] in most_frequent_products:\n",
    "        return row['Product']\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply the function to create the 'Product_label' column\n",
    "complaints_products['Product_label'] = complaints_products.apply(assign_label, axis=1)\n",
    "complaints_products = complaints_products.rename(columns = {'Consumer complaint narrative':'Complaint'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning\n",
    "# Add 'xxxx' to the list of stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.add('xxxx')\n",
    "\n",
    "\n",
    "def text_clean(df,column_name):\n",
    "    df['cleaned_text'] = df[column_name]\n",
    "    \n",
    "    # Normalize text\n",
    "    df['cleaned_text'] = df['cleaned_text'].str.lower()\n",
    "    \n",
    "    # Remove unicode chars + all word formations with 'xxxx' inside\n",
    "    pattern = r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?|(\\w*\\d*xxxx\\d*\\w*|xx\\/xx\\/\\w*\\d*)\"\n",
    "\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: re.sub(pattern, \"\", x))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: \" \".join(word for word in x.split() if word not in stop))\n",
    "    \n",
    "    # Perfrom lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: \" \".join(lemmatizer.lemmatize(word) for word in x.split()))\n",
    "    \n",
    "    return df.head(5)\n",
    "\n",
    "text_clean(complaints_products,'Complaint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the columns that are unique to df2\n",
    "additional_columns = [col for col in complaints.columns if col not in complaints_products.columns]\n",
    "\n",
    "# Concatenate the DataFrames, keeping only the unique columns from df2\n",
    "result_df = pd.concat([complaints_products, complaints[additional_columns]], axis=1)\n",
    "\n",
    "# Save processed data to csv\n",
    "result_df.to_csv('processed_complaints.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reread data\n",
    "complaints_products = pd.read_csv('processed_complaints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Complaint</th>\n",
       "      <th>Product</th>\n",
       "      <th>Product_label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To whom it may concern, There is an account th...</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>may concern account opened name without consen...</td>\n",
       "      <td>To whom it may concern, There is an account th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In  XXXX , I was stuck on high deductible medi...</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>stuck high deductible medical plan son incurre...</td>\n",
       "      <td>In  XXXX , I was stuck on high deductible medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A company named XXXX XXXX XXXX sent me a lette...</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>company named sent letter demanding payment un...</td>\n",
       "      <td>A company named XXXX XXXX XXXX sent me a lette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This account was paid by me even though I was ...</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>account paid even though provided proof owed d...</td>\n",
       "      <td>This account was paid by me even though I was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THIS DEBT COLLECTOR IS ADVERTISING TO COERCE P...</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>debt collector advertising coerce payment debt...</td>\n",
       "      <td>THIS DEBT COLLECTOR IS ADVERTISING TO COERCE P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>Hi, I had fraud committed against my account s...</td>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>hi fraud committed account several time citiba...</td>\n",
       "      <td>Hi, I had fraud committed against my account s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>I have a FDIC account in my HSA ( health savin...</td>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>fdic account hsa health saving account optumba...</td>\n",
       "      <td>I have a FDIC account in my HSA ( health savin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>In  XXXX   XXXX , I saw an online offer fro m ...</td>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>saw online offer fro citibank th stated open s...</td>\n",
       "      <td>In  XXXX   XXXX , I saw an online offer fro m ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>This compliant is on Netspend deliberate failu...</td>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>compliant netspend deliberate failure apply ad...</td>\n",
       "      <td>This compliant is on Netspend deliberate failu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>On XX/XX/XXXX, I deposited two large checks th...</td>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>deposited two large check received checking ac...</td>\n",
       "      <td>On XX/XX/XXXX, I deposited two large checks th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119999 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Complaint  \\\n",
       "0       To whom it may concern, There is an account th...   \n",
       "1       In  XXXX , I was stuck on high deductible medi...   \n",
       "2       A company named XXXX XXXX XXXX sent me a lette...   \n",
       "3       This account was paid by me even though I was ...   \n",
       "4       THIS DEBT COLLECTOR IS ADVERTISING TO COERCE P...   \n",
       "...                                                   ...   \n",
       "119995  Hi, I had fraud committed against my account s...   \n",
       "119996  I have a FDIC account in my HSA ( health savin...   \n",
       "119997  In  XXXX   XXXX , I saw an online offer fro m ...   \n",
       "119998  This compliant is on Netspend deliberate failu...   \n",
       "119999  On XX/XX/XXXX, I deposited two large checks th...   \n",
       "\n",
       "                            Product                Product_label  \\\n",
       "0                   Debt collection              Debt collection   \n",
       "1                   Debt collection              Debt collection   \n",
       "2                   Debt collection              Debt collection   \n",
       "3                   Debt collection              Debt collection   \n",
       "4                   Debt collection              Debt collection   \n",
       "...                             ...                          ...   \n",
       "119995  Checking or savings account  Checking or savings account   \n",
       "119996  Checking or savings account  Checking or savings account   \n",
       "119997  Checking or savings account  Checking or savings account   \n",
       "119998  Checking or savings account  Checking or savings account   \n",
       "119999  Checking or savings account  Checking or savings account   \n",
       "\n",
       "                                             cleaned_text  \\\n",
       "0       may concern account opened name without consen...   \n",
       "1       stuck high deductible medical plan son incurre...   \n",
       "2       company named sent letter demanding payment un...   \n",
       "3       account paid even though provided proof owed d...   \n",
       "4       debt collector advertising coerce payment debt...   \n",
       "...                                                   ...   \n",
       "119995  hi fraud committed account several time citiba...   \n",
       "119996  fdic account hsa health saving account optumba...   \n",
       "119997  saw online offer fro citibank th stated open s...   \n",
       "119998  compliant netspend deliberate failure apply ad...   \n",
       "119999  deposited two large check received checking ac...   \n",
       "\n",
       "                             Consumer complaint narrative  \n",
       "0       To whom it may concern, There is an account th...  \n",
       "1       In  XXXX , I was stuck on high deductible medi...  \n",
       "2       A company named XXXX XXXX XXXX sent me a lette...  \n",
       "3       This account was paid by me even though I was ...  \n",
       "4       THIS DEBT COLLECTOR IS ADVERTISING TO COERCE P...  \n",
       "...                                                   ...  \n",
       "119995  Hi, I had fraud committed against my account s...  \n",
       "119996  I have a FDIC account in my HSA ( health savin...  \n",
       "119997  In  XXXX   XXXX , I saw an online offer fro m ...  \n",
       "119998  This compliant is on Netspend deliberate failu...  \n",
       "119999  On XX/XX/XXXX, I deposited two large checks th...  \n",
       "\n",
       "[119999 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complaints_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/envs/scrapy-env/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "\n",
    "complaints_products = complaints_products.dropna()\n",
    "\n",
    "# Calculate token counts for each document\n",
    "tokenizer = RegexpTokenizer(r'\\w+') # tokenizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(ngram_range = (1, 3), # bigrams and trigrams added\n",
    "                                max_df = 0.9,\n",
    "                                min_df = 0.1,\n",
    "                                tokenizer = tokenizer.tokenize\n",
    ")\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(complaints_products[\"cleaned_text\"])\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "X = tf  # Feature matrix (TF matrix)\n",
    "y = complaints_products[\"Product\"]  # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = MaxAbsScaler() # Scale the features by dividing each feature by the maximum absolute value of that feature.\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20],   # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='balanced_accuracy')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10}\n"
     ]
    }
   ],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for Decision Tree: {'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Accuracy of the best Decision Tree model on the test set: 0.8142083333333333\n",
      "Balanced Accuracy of the best Decision Tree model on the test set: 0.8137400446906501\n",
      "Classification Report:\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "Checking or savings account       0.77      0.81      0.79      5920\n",
      "Credit card or prepaid card       0.76      0.74      0.75      5926\n",
      "            Debt collection       0.83      0.83      0.83      6086\n",
      "                   Mortgage       0.90      0.88      0.89      6068\n",
      "\n",
      "                   accuracy                           0.81     24000\n",
      "                  macro avg       0.81      0.81      0.81     24000\n",
      "               weighted avg       0.81      0.81      0.81     24000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4794  742  228  156]\n",
      " [ 884 4387  534  121]\n",
      " [ 300  430 5029  327]\n",
      " [ 264  180  293 5331]]\n",
      "ROC AUC of the best Decision Tree model on the test set (ovr): 0.875936601238493\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Best model for prediction\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Best Hyperparameters for Decision Tree:\", best_params)\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of the best Decision Tree model on the test set:\", accuracy)\n",
    "\n",
    "# Calculate balanced accuracy\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"Balanced Accuracy of the best Decision Tree model on the test set:\", balanced_acc)\n",
    "\n",
    "# Generate the classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "y_test_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "y_pred_bin = label_binarize(y_pred, classes=np.unique(y_test))\n",
    "\n",
    "# Calculate ROC AUC using the 'ovr' strategy\n",
    "roc_auc = roc_auc_score(y_test_bin, y_pred_bin, multi_class='ovr')\n",
    "print(\"ROC AUC of the best Decision Tree model on the test set (ovr):\", roc_auc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of 0         may concern account opened name without consen...\n",
       "1         stuck high deductible medical plan son incurre...\n",
       "2         company named sent letter demanding payment un...\n",
       "3         account paid even though provided proof owed d...\n",
       "4         debt collector advertising coerce payment debt...\n",
       "                                ...                        \n",
       "119995    hi fraud committed account several time citiba...\n",
       "119996    fdic account hsa health saving account optumba...\n",
       "119997    saw online offer fro citibank th stated open s...\n",
       "119998    compliant netspend deliberate failure apply ad...\n",
       "119999    deposited two large check received checking ac...\n",
       "Name: cleaned_text, Length: 120000, dtype: object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complaints_products[\"cleaned_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing X col with bigrams also\n",
    "from nltk import bigrams\n",
    "\n",
    "# Form bigrams\n",
    "complaints_products['cleaned_text'] = complaints_products['cleaned_text'].astype('str')\n",
    "complaints_products['tokenized_text'] = complaints_products['cleaned_text'].apply(lambda x: x.split(\" \"))\n",
    "complaints_products['tokenized_text'] = complaints_products['tokenized_text'].apply(lambda x: x + [' '.join(b) for b in bigrams(x)])\n",
    "\n",
    "# Assuming 'complaints_products' DataFrame has a 'Product' column\n",
    "# One-hot encode the 'Product' column\n",
    "one_hot_encoded = pd.get_dummies(complaints_products['Product'])\n",
    "\n",
    "# Convert boolean values to integers (0 or 1)\n",
    "one_hot_encoded = one_hot_encoded.astype(int)\n",
    "\n",
    "# Concatenate the one-hot encoded columns to the original DataFrame\n",
    "complaints_products_encoded = pd.concat([complaints_products, one_hot_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 10 10 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy (val): 0.86\n",
      "AUC (val): 0.96\n",
      "Parameters: 10 10 2924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy (val): 0.81\n",
      "AUC (val): 0.94\n",
      "Parameters: 10 10 29240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy (val): 0.69\n",
      "AUC (val): 0.87\n",
      "Parameters: 10 30 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy (val): 0.86\n",
      "AUC (val): 0.96\n",
      "Parameters: 10 30 2924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy (val): 0.81\n",
      "AUC (val): 0.94\n",
      "Parameters: 10 30 29240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy (val): 0.52\n",
      "AUC (val): 0.76\n",
      "Parameters: 30 10 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy (val): 0.88\n",
      "AUC (val): 0.97\n",
      "Parameters: 30 10 2924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy (val): 0.85\n",
      "AUC (val): 0.96\n",
      "Parameters: 30 10 29240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy (val): 0.74\n",
      "AUC (val): 0.91\n",
      "Parameters: 30 30 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy (val): 0.88\n",
      "AUC (val): 0.97\n",
      "Parameters: 30 30 2924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy (val): 0.82\n",
      "AUC (val): 0.95\n",
      "Parameters: 30 30 29240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_59658/2315469417.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy (val): 0.52\n",
      "AUC (val): 0.75\n"
     ]
    }
   ],
   "source": [
    "import tomotopy as tp \n",
    "import math\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Getting X and y\n",
    "X = complaints_products_encoded['tokenized_text']\n",
    "y = complaints_products_encoded.iloc[:, -4:]\n",
    "\n",
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "results = []\n",
    "skf = StratifiedKFold(n_splits=5, random_state=123, shuffle=True)\n",
    "\n",
    "# Unique ngrams\n",
    "unique_tokens_ngrams = list(set(token for tokens in complaints_products['tokenized_text'] for token in tokens))\n",
    "\n",
    "for k in [10, 30]:  # number of topics\n",
    "    for min_df in [10, 30]:  # DF of tokens to be removed \"from the bottom\"\n",
    "        for rm_top in [0, int(len(unique_tokens_ngrams) * 0.001), int(len(unique_tokens_ngrams) * 0.01)]:  # how many tokens should be removed \"from the top\"\n",
    "\n",
    "            print('Parameters:', k, min_df, rm_top)\n",
    "\n",
    "            balanced_accs = []\n",
    "            aucs = []\n",
    "\n",
    "            for i, (train_index, val_index) in enumerate(skf.split(X_train, y_train.iloc[:,0])):\n",
    "                X_train_a= X_train.iloc[train_index]\n",
    "                X_val= X_train.iloc[val_index]\n",
    "                y_train_a = y_train.iloc[train_index].values\n",
    "                y_val= y_train.iloc[val_index].values\n",
    "\n",
    "                slda = tp.SLDAModel(k=k,\n",
    "                                    min_df=min_df,\n",
    "                                    rm_top=rm_top,\n",
    "                                    vars=['b','b','b','b'],  # we have four topics\n",
    "                                    alpha=0.1,\n",
    "                                    eta=0.01,\n",
    "                                    mu=0,\n",
    "                                    nu_sq=1,\n",
    "                                    glm_param=1,\n",
    "                                    seed=123\n",
    "                                    )\n",
    "\n",
    "                for i in range(len(X_train_a)):\n",
    "                    slda.add_doc(X_train_a.iloc[i], y=y_train_a[i,:])\n",
    "\n",
    "                for i in range(0, 100, 20):\n",
    "                    slda.train(20)\n",
    "\n",
    "                train_values = list(y_train_a)\n",
    "                val_values = list(y_val)\n",
    "\n",
    "                train_estimates = []\n",
    "                for doc in slda.docs:\n",
    "                    estimate = slda.estimate(doc)\n",
    "                    train_estimates.append(estimate)\n",
    "\n",
    "                val_estimates = []\n",
    "                for i in range(len(X_val)):\n",
    "                    slda_val_doc = slda.make_doc(X_val.iloc[i])\n",
    "                    slda.infer(slda_val_doc)\n",
    "                    val_estimates.append(slda.estimate(slda_val_doc))\n",
    "\n",
    "                # Compute balanced accuracy\n",
    "                balanced_acc_val = balanced_accuracy_score(np.argmax(y_val, axis=1),\n",
    "                                                           np.argmax(val_estimates, axis=1))\n",
    "                balanced_accs.append(balanced_acc_val)\n",
    "\n",
    "                # Compute weighted F1 score\n",
    "                auc = roc_auc_score(y_val, val_estimates, multi_class='ovr')\n",
    "                aucs.append(auc)\n",
    "                \n",
    "            results.append([k, min_df, rm_top, round(np.mean(aucs), 2)])\n",
    "            print('Balanced Accuracy (val):', round(np.mean(balanced_accs), 2))\n",
    "            print('AUC (val):', round(np.mean(aucs), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 10, 0, 0.96], [10, 10, 2924, 0.94], [10, 10, 29240, 0.87], [10, 30, 0, 0.96], [10, 30, 2924, 0.94], [10, 30, 29240, 0.76], [30, 10, 0, 0.97], [30, 10, 2924, 0.96], [30, 10, 29240, 0.91], [30, 30, 0, 0.97], [30, 30, 2924, 0.95], [30, 30, 29240, 0.75]]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96000/96000 [00:17<00:00, 5440.44it/s]\n",
      "/var/folders/qb/z1cbz14x723cj5l8lxt1k2f00000gn/T/ipykernel_84636/3186371433.py:28: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  slda.train(20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0000 LL per word: 0.0\n",
      "Iteration: 0020 LL per word: -9.419\n",
      "Iteration: 0040 LL per word: -9.066\n",
      "Iteration: 0060 LL per word: -8.954\n",
      "Iteration: 0080 LL per word: -8.898\n",
      "Iteration: 0100 LL per word: -8.863\n",
      "Iteration: 0120 LL per word: -8.844\n",
      "Iteration: 0140 LL per word: -8.832\n",
      "Iteration: 0160 LL per word: -8.824\n",
      "Iteration: 0180 LL per word: -8.817\n"
     ]
    }
   ],
   "source": [
    "# Best model\n",
    "import tomotopy as tp \n",
    "from tqdm import tqdm\n",
    "slda = tp.SLDAModel(k = 30, # number of topics\n",
    "                    min_df = 30, # DF of tokens to be removed \"from the bottom\"\n",
    "                    rm_top = 0, # how many tokens should be removed \"from the top\"\n",
    "                    vars = ['b','b','b','b'], # indicate linear response variable\n",
    "                    alpha = 0.1,\n",
    "                    eta = 0.01,\n",
    "                    mu = 0,\n",
    "                    nu_sq = 1,\n",
    "                    glm_param = 1,\n",
    "                    seed = 123\n",
    "                   )\n",
    "# adds documents to the corpus\n",
    "\n",
    "# Getting X and y\n",
    "X = complaints_products_encoded['tokenized_text']\n",
    "y = complaints_products_encoded.iloc[:, -4:]\n",
    "\n",
    "# splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "for i in tqdm(range(0, X_train.shape[0])):\n",
    "    slda.add_doc(X_train.iloc[i], y = y_train.iloc[i,:])\n",
    "for i in range(0, 200, 20):\n",
    "    print('Iteration: {:04} LL per word: {:.4}'.format(i, slda.ll_per_word))\n",
    "    slda.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words of topic #0:\n",
      "account : 0.16398397088050842\n",
      "closed : 0.024911701679229736\n",
      "access : 0.012652751989662647\n",
      "checking : 0.012523052282631397\n",
      "close : 0.012429293245077133\n",
      "\n",
      "\n",
      "Top 10 words of topic #1:\n",
      "check : 0.06826106458902359\n",
      "fund : 0.03530901297926903\n",
      "money : 0.025745557621121407\n",
      "bank : 0.025311430916190147\n",
      "account : 0.022151347249746323\n",
      "\n",
      "\n",
      "Top 10 words of topic #2:\n",
      "information : 0.02240683324635029\n",
      "consumer : 0.019022773951292038\n",
      "credit : 0.01816413179039955\n",
      "reporting : 0.01652892306447029\n",
      "account : 0.012349356897175312\n",
      "\n",
      "\n",
      "Top 10 words of topic #3:\n",
      "consumer : 0.032327305525541306\n",
      "15 : 0.019105788320302963\n",
      "usc : 0.01548374630510807\n",
      "15 usc : 0.012361587956547737\n",
      "credit : 0.012204844504594803\n",
      "\n",
      "\n",
      "Top 10 words of topic #4:\n",
      "fee : 0.05983734130859375\n",
      "balance : 0.03777819871902466\n",
      "charge : 0.03155619651079178\n",
      "interest : 0.025294827297329903\n",
      "charged : 0.019571484997868538\n",
      "\n",
      "\n",
      "Top 10 words of topic #5:\n",
      "bank : 0.15010620653629303\n",
      "account : 0.042847611010074615\n",
      "america : 0.033090926706790924\n",
      "bank america : 0.03170546144247055\n",
      "transaction : 0.013216033577919006\n",
      "\n",
      "\n",
      "Top 10 words of topic #6:\n",
      "told : 0.044395845383405685\n",
      "would : 0.04292771965265274\n",
      "called : 0.0325225368142128\n",
      "said : 0.028674183413386345\n",
      "back : 0.01957300864160061\n",
      "\n",
      "\n",
      "Top 10 words of topic #7:\n",
      "debt : 0.044988058507442474\n",
      "collection : 0.03671058639883995\n",
      "credit : 0.03291405364871025\n",
      "company : 0.019940420985221863\n",
      "report : 0.016490397974848747\n",
      "\n",
      "\n",
      "Top 10 words of topic #8:\n",
      "chase : 0.0630139485001564\n",
      "dispute : 0.032534729689359665\n",
      "charge : 0.019182240590453148\n",
      "refund : 0.01775813102722168\n",
      "merchant : 0.016532789915800095\n",
      "\n",
      "\n",
      "Top 10 words of topic #9:\n",
      "mortgage : 0.08666428923606873\n",
      "insurance : 0.052151646465063095\n",
      "company : 0.03749895468354225\n",
      "mr : 0.02077084593474865\n",
      "mortgage company : 0.013787630945444107\n",
      "\n",
      "\n",
      "Top 10 words of topic #10:\n",
      "well : 0.17640596628189087\n",
      "fargo : 0.15339475870132446\n",
      "well fargo : 0.1521168053150177\n",
      "u : 0.04050077497959137\n",
      "u bank : 0.037149157375097275\n",
      "\n",
      "\n",
      "Top 10 words of topic #11:\n",
      "credit : 0.07074811309576035\n",
      "report : 0.05785093456506729\n",
      "account : 0.0365920290350914\n",
      "credit report : 0.030663419514894485\n",
      "reporting : 0.02212509885430336\n",
      "\n",
      "\n",
      "Top 10 words of topic #12:\n",
      "application : 0.023030700162053108\n",
      "document : 0.021411988884210587\n",
      "sale : 0.016675757244229317\n",
      "process : 0.011987489648163319\n",
      "submitted : 0.011579814366996288\n",
      "\n",
      "\n",
      "Top 10 words of topic #13:\n",
      "debt : 0.06007871776819229\n",
      "company : 0.018910374492406845\n",
      "call : 0.01887129805982113\n",
      "calling : 0.009802018292248249\n",
      "collect : 0.009723863564431667\n",
      "\n",
      "\n",
      "Top 10 words of topic #14:\n",
      "complaint : 0.019484538584947586\n",
      "would : 0.010188676416873932\n",
      "issue : 0.008332857862114906\n",
      "information : 0.007388705853372812\n",
      "cfpb : 0.0065807802602648735\n",
      "\n",
      "\n",
      "Top 10 words of topic #15:\n",
      "card : 0.1235233023762703\n",
      "credit : 0.10793613642454147\n",
      "credit card : 0.0546659454703331\n",
      "score : 0.010808819904923439\n",
      "limit : 0.009774328209459782\n",
      "\n",
      "\n",
      "Top 10 words of topic #16:\n",
      "letter : 0.07719933986663818\n",
      "received : 0.040590550750494\n",
      "sent : 0.03951208293437958\n",
      "copy : 0.01474546268582344\n",
      "document : 0.014321709983050823\n",
      "\n",
      "\n",
      "Top 10 words of topic #17:\n",
      "debt : 0.028095873072743416\n",
      "collection : 0.012830686755478382\n",
      "alleged : 0.010299187153577805\n",
      "validation : 0.010240315459668636\n",
      "credit : 0.00984206236898899\n",
      "\n",
      "\n",
      "Top 10 words of topic #18:\n",
      "payment : 0.1484024077653885\n",
      "late : 0.02392691932618618\n",
      "made : 0.020617127418518066\n",
      "due : 0.017109712585806847\n",
      "month : 0.013942915946245193\n",
      "\n",
      "\n",
      "Top 10 words of topic #19:\n",
      "account : 0.031290531158447266\n",
      "citi : 0.02918880432844162\n",
      "citibank : 0.028647400438785553\n",
      "bonus : 0.022382574155926704\n",
      "offer : 0.018441418185830116\n",
      "\n",
      "\n",
      "Top 10 words of topic #20:\n",
      "escrow : 0.047366876155138016\n",
      "tax : 0.044227540493011475\n",
      "amount : 0.023075558245182037\n",
      "mortgage : 0.02077711559832096\n",
      "payment : 0.019389642402529716\n",
      "\n",
      "\n",
      "Top 10 words of topic #21:\n",
      "customer : 0.03504932299256325\n",
      "service : 0.03183159604668617\n",
      "customer service : 0.02395591512322426\n",
      "call : 0.02349214255809784\n",
      "representative : 0.01857616752386093\n",
      "\n",
      "\n",
      "Top 10 words of topic #22:\n",
      "card : 0.03568700700998306\n",
      "fraud : 0.02955963835120201\n",
      "claim : 0.02517305500805378\n",
      "charge : 0.025146041065454483\n",
      "transaction : 0.024219084531068802\n",
      "\n",
      "\n",
      "Top 10 words of topic #23:\n",
      "get : 0.019835343584418297\n",
      "money : 0.01766134425997734\n",
      "dont : 0.012301262468099594\n",
      "time : 0.011839804239571095\n",
      "back : 0.011110847815871239\n",
      "\n",
      "\n",
      "Top 10 words of topic #24:\n",
      "one : 0.15832507610321045\n",
      "capital : 0.1390613317489624\n",
      "capital one : 0.12414674460887909\n",
      "bbva : 0.013640142045915127\n",
      "ally : 0.013628961518406868\n",
      "\n",
      "\n",
      "Top 10 words of topic #25:\n",
      "year : 0.024286648258566856\n",
      "month : 0.01503328699618578\n",
      "time : 0.013495287857949734\n",
      "pay : 0.01150381751358509\n",
      "u : 0.011343146674335003\n",
      "\n",
      "\n",
      "Top 10 words of topic #26:\n",
      "email : 0.04232294112443924\n",
      "number : 0.03597545251250267\n",
      "phone : 0.029645441100001335\n",
      "call : 0.028419150039553642\n",
      "received : 0.018985288217663765\n",
      "\n",
      "\n",
      "Top 10 words of topic #27:\n",
      "law : 0.012872938998043537\n",
      "state : 0.012000090442597866\n",
      "court : 0.009697391651570797\n",
      "attorney : 0.00918734073638916\n",
      "property : 0.00873064249753952\n",
      "\n",
      "\n",
      "Top 10 words of topic #28:\n",
      "loan : 0.07874700427055359\n",
      "mortgage : 0.024883370846509933\n",
      "rate : 0.02177211456000805\n",
      "home : 0.01809537783265114\n",
      "closing : 0.017677413299679756\n",
      "\n",
      "\n",
      "Top 10 words of topic #29:\n",
      "loan : 0.045922767370939255\n",
      "mortgage : 0.035192202776670456\n",
      "modification : 0.02968522720038891\n",
      "foreclosure : 0.016574563458561897\n",
      "home : 0.015496528707444668\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in range(slda.k):\n",
    "    print('Top 10 words of topic #{}:'.format(k))\n",
    "    for token, weight in slda.get_topic_words(k, top_n = 5):\n",
    "        print(token, ':', weight)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slda.save('optimal_slda.bin', full=True)\n",
    "import tomotopy as tp \n",
    "slda = tp.SLDAModel().load('optimal_slda.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24000/24000 [04:26<00:00, 90.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8760716073680765\n",
      "0.9694335461723224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Getting X and y\n",
    "X = complaints_products_encoded['tokenized_text']\n",
    "y = complaints_products_encoded.iloc[:, -4:]\n",
    "\n",
    "# splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "test_values = y_test\n",
    "test_estimates = []\n",
    "\n",
    "for i in tqdm(range(y_test.shape[0])):\n",
    "    slda_test_doc = slda.make_doc(X_test.iloc[i])\n",
    "    slda.infer(slda_test_doc)\n",
    "    test_estimates.append(slda.estimate(slda_test_doc))\n",
    "\n",
    "balanced_acc_val = balanced_accuracy_score(np.argmax(test_values, axis=1), np.argmax(test_estimates, axis=1))\n",
    "# Calculate ROC AUC for each class\n",
    "\n",
    "roc_auc = roc_auc_score(test_values, test_estimates, multi_class='ovr')\n",
    "print(balanced_acc_val)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For inner array 1, top four indices are [15 29  9  7 13  5] and values are [-23.725807  -15.979474  -13.451655  -12.468449  -12.3006935  12.151494 ].\n",
      "For inner array 2, top four indices are [15  9 29 28 13  7] and values are [ 23.855532 -17.936125 -16.538229 -14.900158 -13.661053 -12.576155].\n",
      "For inner array 3, top four indices are [ 7 13 15 29  5  9] and values are [ 18.106134  16.923796 -12.485664 -11.769079 -11.487708 -10.024684].\n",
      "For inner array 4, top four indices are [ 9 29 28  7 15 20] and values are [ 21.365965  19.198513  16.235477 -15.895102 -14.031957  13.761041].\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_top_four_elements(array_of_arrays):\n",
    "    result = []\n",
    "\n",
    "    for inner_array in array_of_arrays:\n",
    "        absolute_values = np.abs(inner_array)\n",
    "        sorted_indices = np.argsort(absolute_values)[::-1][:6]\n",
    "        \n",
    "        top_three_indices = sorted_indices\n",
    "        top_three_values = inner_array[top_three_indices]\n",
    "\n",
    "        result.append({\n",
    "            'indices': top_three_indices,\n",
    "            'values': top_three_values\n",
    "        })\n",
    "\n",
    "    return result\n",
    "\n",
    "result = get_top_four_elements(slda.get_regression_coef())\n",
    "\n",
    "for i, entry in enumerate(result):\n",
    "    print(f\"For inner array {i+1}, top four indices are {entry['indices']} and values are {entry['values']}.\")\n",
    "\n",
    "# 7, 9, 13, 15, 28, 29"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "862.3262459682252"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complaints_products['cleaned_text'].apply(lambda x: len(x)).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.dimensionality import BaseDimensionalityReduction\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "X = complaints_products['cleaned_text'].astype(str)\n",
    "y = complaints_products['Product']\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Create a 1/100 sample with the same proportions of classes as the original dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded.astype(int), test_size=0.2, random_state=123)\n",
    "\n",
    "# main difference: classifier replaces the clustering step\n",
    "empty_dimensionality_model = BaseDimensionalityReduction()\n",
    "clf = LogisticRegression()\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words = True)\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    umap_model = empty_dimensionality_model,\n",
    "    hdbscan_model = clf,\n",
    "    ctfidf_model = ctfidf_model,\n",
    "    language = 'english'\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "X = complaints_products['cleaned_text'].astype(str)\n",
    "y = complaints_products['Product']\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Create a 1/100 sample with the same proportions of classes as the original dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded.astype(int), test_size=0.2, random_state=123)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 0, 2: 3, 3: 2}\n"
     ]
    }
   ],
   "source": [
    "mappings = topic_model.topic_mapper_.get_mappings()\n",
    "print(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/envs/scrapy-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning:\n",
      "\n",
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save_object(topic_model, 'outputs_new/topic_model.pkl')\n",
    "# save_object(topics, 'outputs_new/topics.pkl')\n",
    "# save_object(probs, 'outputs_new/probs.pkl')\n",
    "\n",
    "with open(\"outputs_new/topic_model.pkl\", \"rb\") as fp:\n",
    "     topic_model = pickle.load(fp)\n",
    "with open(\"outputs_new/topics.pkl\", \"rb\") as fp:\n",
    "     topics = pickle.load(fp)\n",
    "with open(\"outputs_new/probs.pkl\", \"rb\") as fp:\n",
    "     probs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Checking or savings account', 'Credit card or prepaid card',\n",
       "       'Debt collection', 'Mortgage'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>24107</td>\n",
       "      <td>0_card_credit_charge_account</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>24044</td>\n",
       "      <td>1_account_bank_check_money</td>\n",
       "      <td>Checking or savings account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>24012</td>\n",
       "      <td>2_mortgage_loan_payment_home</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>23837</td>\n",
       "      <td>3_debt_collection_credit_report</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                             Name                        Class\n",
       "0      0  24107     0_card_credit_charge_account  Credit card or prepaid card\n",
       "1      1  24044       1_account_bank_check_money  Checking or savings account\n",
       "2      2  24012     2_mortgage_loan_payment_home                     Mortgage\n",
       "3      3  23837  3_debt_collection_credit_report              Debt collection"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating topic representations (of the last batch)\n",
    "\n",
    "# map input to topics\n",
    "mappings = topic_model.topic_mapper_.get_mappings()\n",
    "mappings = {value: label_encoder.classes_[key] for key, value in mappings.items()}\n",
    "\n",
    "# assign original classes to our topics\n",
    "df = topic_model.get_topic_info()\n",
    "df[\"Class\"] = df.Topic.map(mappings)\n",
    "df[['Topic','Count','Name','Class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24000/24000 [15:05<00:00, 26.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "mapping = {0: 1, 1: 0, 2: 3, 3: 2}\n",
    "test_estimates = []\n",
    "\n",
    "for doc in tqdm(X_test):\n",
    "    topic, _ = topic_model.transform(doc)\n",
    "    mapped_topic = mapping[topic[0]]\n",
    "    test_estimates.append(mapped_topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "encoded_classes = [0, 1, 2, 3]\n",
    "\n",
    "# Reshape the array to a column vector\n",
    "encoded_test = np.array(y_test).reshape(-1, 1)\n",
    "encoded_pred = np.array(test_estimates).reshape(-1, 1)\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "\n",
    "# Fit and transform the data\n",
    "onehot_encoded_test = onehot_encoder.fit_transform(encoded_test)\n",
    "onehot_encoded_estimates = onehot_encoder.transform(encoded_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (test): 0.9333254167388831\n",
      "Balanced Acc (test): 0.8999673731974861\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, f1_score, roc_auc_score\n",
    "AUC = roc_auc_score(onehot_encoded_test, onehot_encoded_estimates, multi_class='ovr')\n",
    "BA_score = balanced_accuracy_score(y_test, test_estimates)\n",
    "print('AUC (test):', AUC)\n",
    "print('Balanced Acc (test):', BA_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Model Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
